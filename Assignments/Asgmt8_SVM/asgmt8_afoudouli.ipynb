{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Assignment 8\n",
    "\n",
    "In this assignment, the Credit Card Fraud Detection dataset is used that can be found on [kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud).\n",
    "\n",
    "In this notebook a classification task to fraudulent and non fraudelent users is done ising support vector machines (SVMs).  \n",
    "Also techniques to handle imbalanced data are implemented.\n",
    "The first part of the workbook focuses on handling imbalanced data. The algorithm and the tuning part starts here (TODO: add link to specific cell)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed install packages uncommenting and executing the following commands\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import os \n",
    "\n",
    "os.chdir('C:/Users/anast/OneDrive/Desktop/MSc/MachineLearning/Assignments/Asgmt8_SVM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'creditcard.csv'\n",
    "\n",
    "data = pd.read_csv(data_file)"
   ]
  },
  {
   "source": [
    "**Scaling**  \n",
    "Time and amount variables need to be scaled. The rest of the variables (the PCs) are already scaled \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The target variable is extremely imbalanced. Only 0.17% out of all the transactions of the dataset are fraudulent. This problem, may lead any model to overfit the non-fraudulent examples, being unable to recognise fraud. There are different ways to handle such issues.  \n",
    "Here I will experiment with:\n",
    "* Undersampling\n",
    "* Oversampling\n",
    "* Combination of both\n",
    "\n",
    "Some nice guides can be found on [DataCamp](https://www.datacamp.com/community/tutorials/diving-deep-imbalanced-data?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034349&utm_targetid=aud-390929969673:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=9061579&gclid=CjwKCAiAq8f-BRBtEiwAGr3Dgc65y799jXfSyX1UAugegeLHUDk7lb6izpB-coR1udmOQvHoN76s2xoCpg8QAvD_BwE), [KDnuggets](https://www.kdnuggets.com/2020/01/5-most-useful-techniques-handle-imbalanced-datasets.html) and [Machine Learning Mastery](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Under-sampling\n",
    "\n",
    "**Random under-sampling** \n",
    " \n",
    "In random under-sampling, the only a subset of the majority class examples is retained and all the observation of the minority class are retained.\n",
    "\n",
    "**Pros**: Improve the runtime of the model by reducing the number of training data samples when the training set is gigantic.   \n",
    "**Cons**: There is high risk of information loss as only a small subset of the majority class training examples is used.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraud = data[data['Class']==1]\n",
    "data_no_fraud = data[data['Class']==0]\n",
    "\n",
    "fraud_count = data_fraud['Class'].count()\n",
    "\n",
    "# undersample majority class\n",
    "data_no_fraud = resample(data_no_fraud, replace=False, n_samples=int(fraud_count*3), random_state=909)\n",
    "\n",
    "data_undersampled = pd.concat([data_fraud, data_no_fraud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_undersampled['Amount'] = scaler.fit_transform(data_undersampled['Amount'].values.reshape(-1,1))\n",
    "data_undersampled['Time'] = scaler.fit_transform(data_undersampled['Time'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_undersampled.drop(columns='Class')\n",
    "y = data_undersampled['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=133, stratify=y)"
   ]
  },
  {
   "source": [
    "**Tomek Links**  \n",
    "\n",
    "Undersampling can also be achieved using [Tomek links](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.TomekLinks.html). Tomek links are pairs of examples of opposite classes in close vicinity. It is a fairly expensive algorithm since it has to compute pairwise distances between all examples. After this calculation, the majority elements from the Tomek link are removed, thus providing a better decision boundary for a classifier. Samples from the majorith, the minority or both classes can be removed.  \n",
    "Undersampling can also be performed on the resulting dataset as discussed [here](https://www.hilarispublisher.com/open-access/classification-of-imbalance-data-using-tomek-link-tlink-combined-with-random-undersampling-rus-as-a-data-reduction-method-2229-8711-S1111.pdf)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl = TomekLinks(sampling_strategy='majority')\n",
    "\n",
    "# X_tl, y_tl = tl.fit_resample(X,y)"
   ]
  },
  {
   "source": [
    "### Over-sampling\n",
    "\n",
    "**SMOTE**\n",
    "\n",
    "Synthetic Minority Oversampling Technique (SMOTE) is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.\n",
    "\n",
    "Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = SMOTE(sampling_strategy=0.3, random_state=133, n_jobs=-1)\n",
    "\n",
    "# X_sm, y_sm = sm.fit_resample(X,y)"
   ]
  },
  {
   "source": [
    "**ADASYN**  \n",
    "\n",
    "Adaptive Synthetic Sampling Approach (ADASYN) is a generalized form of the SMOTE algorithm. Again, this algorithm aims to oversample the minority class by generating synthetic instances for it. But the difference here is it considers the density distribution, ri which decides the no. of synthetic instances generated for samples which are difficult to learn. Due to this, it helps in adaptively changing the decision boundaries based on the samples difficult to learn."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada = ADASYN(random_state=133, n_jobs=-1)\n",
    "\n",
    "# X_ada, y_ada = ada.fit_resample(X_train, y_train)"
   ]
  },
  {
   "source": [
    "I will be working with the undersampled data, so that i have less examples to train on and reduce the training time of SVM."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**TODO**  \n",
    "Write a few things about SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [0.1, 10, 0.1, 10, 0.1, 10, 100]\n",
    "kernels = ['poly', 'poly', 'rbf', 'rbf', 'sigmoid', 'sigmoid', 'sigmoid']\n",
    "gammas = [0.2, 6, 0.3, 5, 0.5, 2, 5]\n",
    "degrees = [2, 5, 0, 0, 0, 0, 0]\n",
    "\n",
    "params = [x for x in zip(c_values, kernels, gammas, degrees)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_test, predictions):  \n",
    "    print(f'Accuracy {accuracy_score(y_test, predictions)*100:.2f}%'\n",
    "            f'\\nPrecision {precision_score(y_test, predictions)*100:.2f}% '\n",
    "            f'\\nRecall {recall_score(y_test, predictions)*100:.2f}% '\n",
    "            f'\\nF1 {f1_score(y_test, predictions)*100:.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "SVM trained on parameters: C=0.1, kernel=poly, gamma=0.2, degree=2.\n",
      "The model achieved the following metrics:\n",
      "Accuracy 95.38%\n",
      "Precision 92.36% \n",
      "Recall 88.96% \n",
      "F1 90.62% \n",
      "\n",
      "SVM trained on parameters: C=10, kernel=poly, gamma=6, degree=5.\n",
      "The model achieved the following metrics:\n",
      "Accuracy 94.77%\n",
      "Precision 89.57% \n",
      "Recall 89.57% \n",
      "F1 89.57% \n",
      "\n",
      "SVM trained on parameters: C=0.1, kernel=rbf, gamma=0.3, degree=0.\n",
      "The model achieved the following metrics:\n",
      "Accuracy 74.92%\n",
      "Precision 0.00% \n",
      "Recall 0.00% \n",
      "F1 0.00% \n",
      "\n",
      "SVM trained on parameters: C=10, kernel=rbf, gamma=5, degree=0.\n",
      "The model achieved the following metrics:\n",
      "Accuracy 76.15%\n",
      "Precision 100.00% \n",
      "Recall 4.91% \n",
      "F1 9.36% \n",
      "\n",
      "SVM trained on parameters: C=0.1, kernel=sigmoid, gamma=0.5, degree=0.\n",
      "The model achieved the following metrics:\n",
      "Accuracy 76.77%\n",
      "Precision 53.37% \n",
      "Recall 58.28% \n",
      "F1 55.72% \n",
      "\n",
      "SVM trained on parameters: C=10, kernel=sigmoid, gamma=2, degree=0.\n",
      "The model achieved the following metrics:\n",
      "Accuracy 81.23%\n",
      "Precision 62.73% \n",
      "Recall 61.96% \n",
      "F1 62.35% \n",
      "\n",
      "SVM trained on parameters: C=100, kernel=sigmoid, gamma=5, degree=0.\n",
      "The model achieved the following metrics:\n",
      "Accuracy 76.15%\n",
      "Precision 52.25% \n",
      "Recall 57.06% \n",
      "F1 54.55% \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(params)):\n",
    "    clf = SVC(C=params[i][0], \n",
    "              kernel=params[i][1], \n",
    "              gamma=params[i][2], \n",
    "              degree=params[i][3])\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f'\\nSVM trained on parameters: C={params[i][0]}, kernel={params[i][1]}, gamma={params[i][2]}, degree={params[i][3]}.')\n",
    "    print('The model achieved the following metrics:')\n",
    "    print_metrics(y_test, clf.predict(X_test))"
   ]
  },
  {
   "source": [
    "#### TODO  \n",
    "Make this a notebook of different under-sampling, over-sampling techniques."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}