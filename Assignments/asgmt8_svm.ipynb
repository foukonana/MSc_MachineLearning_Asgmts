{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Assignment 8\n",
    "\n",
    "In this assignment, the Credit Card Fraud Detection dataset is used that can be found on [kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud).\n",
    "\n",
    "In this notebook a classification task to fraudulent and non fraudelent users is done ising support vector machines (SVMs).  \n",
    "Also techniques to handle imbalanced data are implemented.\n",
    "The first part of the workbook focuses on handling imbalanced data. The algorithm part starts [here](#svm_section)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed install packages uncommenting and executing the following commands\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_roc_curve\n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler, NearMiss\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "\n",
    "os.chdir('C:/Users/anast/OneDrive/Desktop/MSc/MachineLearning/Assignments/Asgmt8_SVM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'creditcard.csv'\n",
    "\n",
    "data = pd.read_csv(data_file)"
   ]
  },
  {
   "source": [
    "**Scaling**  \n",
    "Time and amount variables need to be scaled. The rest of the variables (the PCs) are already scaled \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The target variable is extremely imbalanced. Only 0.17% out of all the transactions of the dataset are fraudulent. This problem, may lead any model to overfit the non-fraudulent examples, being unable to recognise fraud. There are different ways to handle such issues.  \n",
    "Here I will experiment with:\n",
    "* Undersampling\n",
    "* Oversampling\n",
    "* Combination of both\n",
    "\n",
    "Some nice guides can be found on [DataCamp](https://www.datacamp.com/community/tutorials/diving-deep-imbalanced-data?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034349&utm_targetid=aud-390929969673:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=9061579&gclid=CjwKCAiAq8f-BRBtEiwAGr3Dgc65y799jXfSyX1UAugegeLHUDk7lb6izpB-coR1udmOQvHoN76s2xoCpg8QAvD_BwE), [KDnuggets](https://www.kdnuggets.com/2020/01/5-most-useful-techniques-handle-imbalanced-datasets.html) and [Machine Learning Mastery](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Under-sampling\n",
    "\n",
    "**Random under-sampling** \n",
    " \n",
    "In random under-sampling, the only a subset of the majority class examples is retained and all the observation of the minority class are retained.\n",
    "\n",
    "**Pros**: Improve the runtime of the model by reducing the number of training data samples when the training set is gigantic.   \n",
    "**Cons**: There is high risk of information loss as only a small subset of the majority class training examples is used.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data['Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "data['Time'] = scaler.fit_transform(data['Time'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns='Class')\n",
    "y = data['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=133, stratify=y)"
   ]
  },
  {
   "source": [
    "### Under-sampling\n",
    "\n",
    "**Random under-sampling** \n",
    " \n",
    "In random under-sampling, the only a subset of the majority class examples is retained and all the observation of the minority class are retained.\n",
    "\n",
    "**Pros**: Improve the runtime of the model by reducing the number of training data samples when the training set is gigantic.   \n",
    "**Cons**: There is high risk of information loss as only a small subset of the majority class training examples is used."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# data_fraud = train_data[train_data['Class']==1]\n",
    "# data_no_fraud = train_data[train_data['Class']==0]\n",
    "\n",
    "# fraud_count = data_fraud['Class'].count()\n",
    "\n",
    "# # undersample majority class\n",
    "# data_no_fraud = resample(data_no_fraud, replace=False, n_samples=int(fraud_count*3), random_state=909)\n",
    "\n",
    "# data_undersampled = pd.concat([data_fraud, data_no_fraud])\n",
    "\n",
    "# X_train = data_undersampled.drop(columns='Class')\n",
    "# y_train = data_undersampled['Class']"
   ]
  },
  {
   "source": [
    "**Tomek Links**  \n",
    "\n",
    "Undersampling can also be achieved using [Tomek links](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.TomekLinks.html). Tomek links are pairs of examples of opposite classes in close vicinity. It is a fairly expensive algorithm since it has to compute pairwise distances between all examples. After this calculation, the majority elements from the Tomek link are removed, thus providing a better decision boundary for a classifier. Samples from the majorith, the minority or both classes can be removed.  \n",
    "Undersampling can also be performed on the resulting dataset as discussed [here](https://www.hilarispublisher.com/open-access/classification-of-imbalance-data-using-tomek-link-tlink-combined-with-random-undersampling-rus-as-a-data-reduction-method-2229-8711-S1111.pdf)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl = TomekLinks(sampling_strategy='majority')\n",
    "\n",
    "# X_tl, y_tl = tl.fit_resample(X,y)"
   ]
  },
  {
   "source": [
    "**Near Miss**  \n",
    "\n",
    "Near Miss is again an undersampling method that select examples based on the distance of majority class examples to minority class examples. NearMiss-1, NearMiss-2 and NearMiss-3 are the three versions of this technique. \n",
    "* NearMiss-1\n",
    "Selects examples from the majority class with the lowest mean distance to the three closest examples of the minority class\n",
    "* NearMiss-2\n",
    "Selects examples from the majority class with the lowest average distance to the three furthest examples of the minority class\n",
    "* NearMiss-3\n",
    "Selects examples from the majority class for each example of the minority class that are closest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miss = NearMiss(sampling_strategy='majority')\n",
    "\n",
    "# X_miss, y_miss = miss.fit_resample(X,y)"
   ]
  },
  {
   "source": [
    "### Over-sampling\n",
    "\n",
    "**SMOTE**\n",
    "\n",
    "Synthetic Minority Oversampling Technique (SMOTE) is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.\n",
    "\n",
    "Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = SMOTE(sampling_strategy=0.3, random_state=133, n_jobs=-1)\n",
    "\n",
    "# X_sm, y_sm = sm.fit_resample(X,y)"
   ]
  },
  {
   "source": [
    "**ADASYN**  \n",
    "\n",
    "Adaptive Synthetic Sampling Approach (ADASYN) is a generalized form of the SMOTE algorithm. Again, this algorithm aims to oversample the minority class by generating synthetic instances for it. But the difference here is it considers the density distribution, ri which decides the no. of synthetic instances generated for samples which are difficult to learn. Due to this, it helps in adaptively changing the decision boundaries based on the samples difficult to learn."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada = ADASYN(random_state=133, n_jobs=-1)\n",
    "\n",
    "# X_ada, y_ada = ada.fit_resample(X_train, y_train)"
   ]
  },
  {
   "source": [
    "**Combining over and under-sampling techniques**  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "under = RandomUnderSampler(sampling_strategy=0.01, random_state=101)\n",
    "miss = NearMiss(sampling_strategy=0.08, n_neighbors=3, n_jobs=-1)\n",
    "tomek = TomekLinks(sampling_strategy='majority', n_jobs=-1)\n",
    "ada = ADASYN(sampling_strategy=0.1, random_state=359, n_jobs=-1)\n",
    "\n",
    "imbalance_pipe = Pipeline(steps=[('under_sampling', under), \n",
    "                                 ('nearmiss', miss),\n",
    "                                 #('tomek_links', tomek), \n",
    "                                 ('ada', ada)\n",
    "                                 ])\n",
    "\n",
    "X_train_resampled, y_train_resampled = imbalance_pipe.fit_resample(X_train, y_train) "
   ]
  },
  {
   "source": [
    "<a id='svm_section'></a>\n",
    "\n",
    "### SVM  \n",
    "Support vector machines (SVMs) are supervised classification algorithms. The classifier seperates the data points by finding the optimal hyperplane with the greatest amout of margin between the existing data points (that constitute the different classes). In other words, the best hyperplane is that whose distance to the nearest element of each class is the largest. Support vectors are the data points that are closest to the hyperplane. These are the most relevant datapoints for the classifier.\n",
    "\n",
    "When data are not linearly seperatable, SVM uses a kernel trick. The kernel takes a low-dimensional input space and transforms it into a higher dimensional space. In other words, you can say that it converts nonseparable problem to separable problems by adding more dimension to it. It is most useful in non-linear separation problem. Kernel trick helps you to build a more accurate classifier.  \n",
    "Common kernels:\n",
    "* Linear kernel\n",
    "* Polynomial kerner\n",
    "* RBF kernel (radian basis function)\n",
    "\n",
    "\n",
    "Note: _Although SVM are considered classification approaches, they can be used in both classification and regression tasks._   \n",
    "Some notes on the maths for SVMs on [analyticsvidhya](https://www.analyticsvidhya.com/blog/2020/10/the-mathematics-behind-svm/) and [Andrew Ng](https://www.youtube.com/watch?v=QKc3Tr7U4Xc)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [0.1, 10, 0.1, 10, 0.1, 10, 100]\n",
    "kernels = ['poly', 'poly', 'rbf', 'rbf', 'sigmoid', 'sigmoid', 'sigmoid']\n",
    "gammas = [0.2, 6, 0.3, 5, 0.5, 2, 5]\n",
    "degrees = [2, 5, 0, 0, 0, 0, 0]\n",
    "\n",
    "params = [x for x in zip(c_values, kernels, gammas, degrees)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "227845 394 0.001729245759178389\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train), sum(y_train), sum(y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:49<00:00,  7.08s/it]\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "precission = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for i in tqdm(range(len(params)), position=0):\n",
    "    clf = SVC(C=params[i][0], \n",
    "              kernel=params[i][1], \n",
    "              gamma=params[i][2], \n",
    "              degree=params[i][3],\n",
    "              max_iter=1000,\n",
    "              tol=0.01)\n",
    "    clf.fit(X_train_resampled, y_train_resampled)\n",
    "    preds = clf.predict(X_test)\n",
    "    accuracy += [accuracy_score(y_test, preds)]\n",
    "    precission += [precision_score(y_test, preds)]\n",
    "    recall += [recall_score(y_test, preds)]\n",
    "    f1 += [f1_score(y_test, preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       C   Kernel  Gamma Degree  Accuracy  Precision    Recall        F1\n",
       "0    0.1     poly    0.2      2  0.929866   0.022777  0.948980  0.044487\n",
       "1   10.0     poly    6.0      5  0.356325   0.002476  0.928571  0.004939\n",
       "2    0.1      rbf    0.3      -  0.998280   0.000000  0.000000  0.000000\n",
       "3   10.0      rbf    5.0      -  0.998385   0.875000  0.071429  0.132075\n",
       "4    0.1  sigmoid    0.5      -  0.945543   0.013286  0.418367  0.025754\n",
       "5   10.0  sigmoid    2.0      -  0.932885   0.007405  0.285714  0.014437\n",
       "6  100.0  sigmoid    5.0      -  0.941329   0.013205  0.448980  0.025656"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C</th>\n      <th>Kernel</th>\n      <th>Gamma</th>\n      <th>Degree</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.1</td>\n      <td>poly</td>\n      <td>0.2</td>\n      <td>2</td>\n      <td>0.929866</td>\n      <td>0.022777</td>\n      <td>0.948980</td>\n      <td>0.044487</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10.0</td>\n      <td>poly</td>\n      <td>6.0</td>\n      <td>5</td>\n      <td>0.356325</td>\n      <td>0.002476</td>\n      <td>0.928571</td>\n      <td>0.004939</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.1</td>\n      <td>rbf</td>\n      <td>0.3</td>\n      <td>-</td>\n      <td>0.998280</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10.0</td>\n      <td>rbf</td>\n      <td>5.0</td>\n      <td>-</td>\n      <td>0.998385</td>\n      <td>0.875000</td>\n      <td>0.071429</td>\n      <td>0.132075</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.1</td>\n      <td>sigmoid</td>\n      <td>0.5</td>\n      <td>-</td>\n      <td>0.945543</td>\n      <td>0.013286</td>\n      <td>0.418367</td>\n      <td>0.025754</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10.0</td>\n      <td>sigmoid</td>\n      <td>2.0</td>\n      <td>-</td>\n      <td>0.932885</td>\n      <td>0.007405</td>\n      <td>0.285714</td>\n      <td>0.014437</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>100.0</td>\n      <td>sigmoid</td>\n      <td>5.0</td>\n      <td>-</td>\n      <td>0.941329</td>\n      <td>0.013205</td>\n      <td>0.448980</td>\n      <td>0.025656</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "results_df = pd.DataFrame(data = {'C': c_values,\n",
    "                                  'Kernel':kernels,\n",
    "                                  'Gamma': gammas,\n",
    "                                  'Degree': [x if x>0 else '-' for x in degrees],\n",
    "                                  'Accuracy': accuracy,\n",
    "                                  'Precision': precission,\n",
    "                                  'Recall': recall,\n",
    "                                  'F1': f1})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose the best svm model is the first one as it has the best recall\n",
    "clf = SVC(C=0.1, \n",
    "          kernel='poly', \n",
    "          gamma=0.2, \n",
    "          degree=2,\n",
    "          max_iter=1000,\n",
    "          tol=0.01)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       1.00      0.93      0.96     56864\n           1       0.02      0.95      0.04        98\n\n    accuracy                           0.93     56962\n   macro avg       0.51      0.94      0.50     56962\nweighted avg       1.00      0.93      0.96     56962\n\nConfusion matrix\n[[52874  3990]\n [    5    93]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))\n",
    "print('Confusion matrix')\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "source": [
    "** Hyperparameter search**\n",
    "\n",
    "For imbalanced datasets, hyperparameter search is not an easy and straingforward procedure. \n",
    "Grid search is usually done with cross validation. Using a validation set that is taken from the undersampled or resamples data, will introduce an overconfidence level to the model. Good results in the validation set may not be followed by good results on the test set. Therefore, the validation set should not be either undersampled or oversampled.  \n",
    "Some interesting notes are on [researchgate](https://www.researchgate.net/post/should_oversampling_be_done_before_or_within_cross-validation) and [stackexchange](https://datascience.stackexchange.com/questions/61858/oversampling-undersampling-only-train-set-only-or-both-train-and-validation-set).\n",
    "\n",
    "TODO: Create cross validation, with the correct validation data.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'C': np.arange(0.1,0.5,0.1),\n",
    "#           'gamma':np.arange(0.1,0.5,0.1),\n",
    "#           'degree':range(1,4)\n",
    "#           }\n",
    "\n",
    "# scoring = {'Accuracy': make_scorer(accuracy_score),\n",
    "#            'Precision': make_scorer(precision_score),\n",
    "#            'Recall': make_scorer(recall_score),\n",
    "#            'F1': make_scorer(f1_score),\n",
    "#            }\n",
    "\n",
    "\n",
    "# clf = SVC(kernel='poly', max_iter=1000, tol=0.01)\n",
    "\n",
    "# grid = GridSearchCV(clf, \n",
    "#                     param_grid=params,\n",
    "#                     cv=3,\n",
    "#                     scoring=scoring, \n",
    "#                     refit='F1', \n",
    "#                     return_train_score=True,\n",
    "#                     verbose=1,\n",
    "#                     n_jobs=-1)\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# print(grid.best_score_)\n",
    "# print(grid.best_estimator_)"
   ]
  },
  {
   "source": [
    "#### TODO  \n",
    "Make this a notebook of different under-sampling and over-sampling techniques."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}