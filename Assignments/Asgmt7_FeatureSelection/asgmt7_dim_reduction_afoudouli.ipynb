{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Assignment 7 \n",
    "### Dimensionality reduction\n",
    "\n",
    "In this assignment, the HTRU2 dataset is used that can be found on [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/HTRU2).\n",
    "\n",
    "In this workbook, dimensionality reduction techniques are implemented."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Introduction\n",
    "Introduction on HTRU2 dataset problem can be found on asgmt7_feat_importance_afoudouli.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Dimensionality reduction refers to the techniques that are used and are able to transform a high dimensional space into a lower dimensional one withouth the loss of much information.  \n",
    "In Machine Learning, dimesnionality reduction can be classified into data preparation techniques. In can improve an algorithms results, as fewer parameters lead to faster train times, less noise in data and possibly better generalization.  \n",
    "\n",
    "There are various dimensionality reduction techniques. They can be classified into two categories:\n",
    "* Linear algebra methods  \n",
    "Examples include PCA, SVD, LDA \n",
    "* Manifold learning methods   \n",
    "Examples include tsne, UMAP, autoencoders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed install packages uncommenting the following commands\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "os.chdir('C:/Users/anast/OneDrive/Desktop/MSc/MachineLearning/Assignments/Asgmt7_FeatureSelection/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'HTRU2/HTRU_2.csv'\n",
    "\n",
    "data = pd.read_csv(data_file, header=None)\n",
    "\n",
    "\n",
    "feature_names = [\"Mean of the integrated profile\",\n",
    "\t\"Standard deviation of the integrated profile\",\n",
    "\t\"Excess kurtosis of the integrated profile\",\n",
    "\t\"Skewness of the integrated profile\",\n",
    "\t\"Mean of the DM-SNR curve\",\n",
    "\t\"Standard deviation of the DM-SNR curve\",\n",
    "\t\"Excess kurtosis of the DM-SNR curve\",\n",
    "\t\"Skewness of the DM-SNR curve\"]\n",
    "\n",
    "data.columns = feature_names + [\"target_class\"]"
   ]
  },
  {
   "source": [
    "### Principal Component Analysis\n",
    "Principal Component Analysis, or PCA for short, is a method for reducing the dimensionality of data.  \n",
    "In its essence, PCA is not a dimensionality reduction algorithm but the properties of principal components make it a data scientist' fevourite when trying to reduce the number of features.  \n",
    "PCA transform the original N-dimensional dataset, into a set of N new features called principal components. The information contained in a column is the amount of variance it contains. The primary objective of Principal Components is to represent the information in the dataset with as low number of columns as possible. PCs are formed in such a way that the first Principal Component (PC1) explains more variance in original data compared to PC2. Likewise, PC2 explains more than PC3, and so on. Each of the PCs contains weights (called loadings) which are actuale the eigenvectors of the original data X.\n",
    "\n",
    "The problem can be easier understood on a 2D space.  \n",
    "Using these, we want to find aprojection that can better \"represent\" the information in the data. This new column can be thought of as a line that passes through these points. Such a line can be represented as a linear combination of the two columns and explains the maximum variation present in these two columns. It should be in a direction that minimizes the perpendicular distance of each point from the line.\n",
    "\n",
    "<iframe src=\"https://gifer.com/embed/H7zW\" width=480 height=192.000 frameBorder=\"0\" allowFullScreen></iframe><p><a href=\"https://gifer.com\">via GIFER</a></p>\n",
    "\n",
    "Some great tutorials can be found on [umetrics](https://blog.umetrics.com/what-is-principal-component-analysis-pca-and-how-it-is-used), [built in](https://builtin.com/data-science/step-step-explanation-principal-component-analysis), [Machine Learning Plus](https://www.machinelearningplus.com/machine-learning/principal-components-analysis-pca-better-explained/)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns = 'target_class')\n",
    "y = data['target_class'].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=556, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "clf = XGBClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dim_pipe = Pipeline([('scaling', scaler),\n",
    "                          ('classifier', clf)])\n",
    "\n",
    "full_dim_pipe.fit(X_train, y_train);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "\n",
    "pca_pipe = Pipeline([('scaling', scaler),\n",
    "                     ('pca', pca),\n",
    "                     ('classifier', clf)])\n",
    "\n",
    "pca_pipe.fit(X_train, y_train); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}